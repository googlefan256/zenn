---
title: "LLMの性能評価をしよう(ロングコンテキスト編)"
emoji: "🌾"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["llm"]
published: false
---
# はじめに
今日のLLMにとって、ロングコンテキスト(長文対応)性能は以前よりも大幅に重視されるものとなりました。
様々なモデルが"長時間思考出来ること"をメリットとしてアピールしていますが、実際のところ、ロングコンテキストに対応し、長時間にわたる推論を齟齬なく行えるモデルはそこまで多くありません。
これまで有志による新規モデルへの長コンテキスト性能評価が断続的に行われていますが、その中で核となる`Needle in a Haystack`というベンチマークの公式コードが非常に読みにくく理解しにくかったため、それを簡潔に実装したものを公開し、`Needle in a Haystack`の布教、仕組みの解説を行います。

# なぜNeedle in a Haystackがいいのか
一般的なLLMに対するベンチマークは、質問と回答のペアを暗記させることで容易に不正することが可能です。LLM各社が意識しなくとも、事前学習データにQAペアとして含まれることで、想定外に高得点を取る可能性があります。
一方で、Needle in a Haystackは、QAペアをその場で柔軟に変更することや、余計な情報をシャッフルすることで事前学習による対策が非常に困難であり、不正に点数が高くなることが少ないです。
逆に言えば開発会社にとってはベンチマークスコアの上昇が狙いにくいため、公式リリースポストなどで引用されることは非常に稀です。

# Needle in a Haystackの仕組み
`Needle in a Haystack`は、長文の中から特定の情報を正確に抽出できるかを評価するベンチマークです。
具体的には以下の手順で評価を行います。

1. 測定を行う際には、`質問`及び`回答`、そして大量の余計な情報を含む`コンテキスト`を用意します。
2. 評価対象のLLMに、固定長に近い長さの`コンテキスト`に対し、指定された位置に`回答`を埋め込んだ質問を与えます。
3. 評価対象のLLMの回答を別のLLMに評価させることで回答が正しいかを判定し、0-10の得点を付与します。
4. 上記の手順をコンテキスト長や埋め込み位置を変えて繰り返し、最終的な得点を算出します。

# 測定してみよう
Needle in a Haystackは、[こちらのレポジトリ](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)が公式のコードとなっているのですが、大変読みにくかったので[こちら](https://github.com/neodyland/needle-in-a-haystack)に再実装しました。
実行には`localhost:8000`(引数で設定可能)などにて動作しているOpenAI api互換のLLMが必要です。

1. `git clone https://github.com/neodyland/needle-in-a-haystack.git` でレポジトリをクローンします。
2. `uv sync` で依存関係をインストールします。
3. `uv run main.py --bench-openai-api-key <API KEY> --bench-openai-api-base <APIベースURL> --bench-openai-model <測定するモデル名> --openai-tokenizer <tiktokenのトークナイザ(または--transformers-tokenizerにtransformersのモデル名)>` で測定を行います。
4. `results.png`に結果が出力されます。

# 最後に
LLMを堅実に使うには、LLMの正確な性能評価は欠かせません。
Needle in a Haystackなどのベンチマーク測定を通じて、本当の意味で自分の用途に合うLLM探しをしましょう。